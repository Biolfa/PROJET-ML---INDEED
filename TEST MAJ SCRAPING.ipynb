{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "dbnames = client.list_database_names()\n",
    "if \"indeed_db\" in dbnames:\n",
    "    update = True\n",
    "else:\n",
    "    update = False\n",
    "\n",
    "db = client.indeed_db\n",
    "job_offers_collec = db.job_offers\n",
    "\n",
    "try:\n",
    "    salary_df = pd.read_csv('salary_indeed.csv')\n",
    "except FileNotFoundError:\n",
    "    salary_df = pd.DataFrame(columns=['_id', 'Title', 'Company', 'Location',\n",
    "                                      'Salary', 'Description', 'Date',\n",
    "                                      'Job_Search', 'Department_Search'])\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "professions = [\"title%3A+data+scientist\", \"title%3A+data+analyst\",\n",
    "               \"title%3A+data+architect\", \"title%3A+data+engineer\",\n",
    "               \"informatique+title%3A+développeur\", \"title%3A+devops\",\n",
    "               \"title%3A+software+engineer\"]\n",
    "# 75 = Paris ; Gironde = Bordeaux ; Rhône = Lyon\n",
    "# Loire-Atlantique = Nantes ; Haute-Garonne = Toulouse\n",
    "# 75 à la place de Paris car ce dernier donne Montreuil par ex.\n",
    "departments = [\"75\", \"Gironde\", \"Rhône\", \"Loire-Atlantique\", \"Haute-Garonne\"]\n",
    "\n",
    "for profession in professions:\n",
    "    for dpt in departments:\n",
    "        if update:\n",
    "            # Add &fromage=x at the end of the url to look for the last x days\n",
    "            url = 'https://www.indeed.fr/jobs?q={}&l={}&sort=date&fromage=7'\\\n",
    "                    .format(profession, dpt)\n",
    "        else:\n",
    "            url = 'https://www.indeed.fr/jobs?q={}&l={}&sort=date'\\\n",
    "                    .format(profession, dpt)\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(4)\n",
    "\n",
    "        first_page = True\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                all_jobs = driver.find_elements_by_class_name('result')\n",
    "\n",
    "                for job in all_jobs:\n",
    "                    result_html = job.get_attribute('innerHTML')\n",
    "                    soup = BeautifulSoup(result_html, 'lxml')\n",
    "\n",
    "                    id_ = job.get_attribute('id')\n",
    "\n",
    "                    date = soup.find(class_=\"date\")\n",
    "                    if date is not None:\n",
    "                        date = date.text\n",
    "                        if any(word in date.lower()\n",
    "                                for word in [\"instant\", \"seconde\", \"minute\",\n",
    "                                             \"heure\", \"aujourd'hui\"]):\n",
    "                            date = datetime.date.today()\n",
    "                        else:\n",
    "                            number = int(re.findall(r'\\d+', date)[0])\n",
    "                            if \"jour\" in date:\n",
    "                                date = (datetime.date.today()\n",
    "                                        - datetime.timedelta(days=number))\n",
    "                            elif \"mois\" in date:\n",
    "                                # 1 month ~ 4.35 weeks\n",
    "                                # ==> x months ~ 4.35 * x weeks\n",
    "                                date = (datetime.date.today()\n",
    "                                        - datetime.timedelta(weeks=4.35*number))\n",
    "\n",
    "                        try:\n",
    "                            date = date.strftime(r\"%d/%m/%Y\")\n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "\n",
    "                    location = soup.find(class_=\"location\")\n",
    "                    if location is not None:\n",
    "                        location = location.text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "                    company = soup.find(class_=\"company\")\n",
    "                    if company is not None:\n",
    "                        company = company.text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "                    salary = soup.find(class_=\"salary\")\n",
    "                    if salary is not None:\n",
    "                        salary = salary.text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "                    sum_div = job.find_element_by_class_name(\"summary\")\n",
    "                    sum_div.click()\n",
    "                    driver.implicitly_wait(4)\n",
    "\n",
    "                    job_desc = driver.find_element_by_id('vjs-desc')\\\n",
    "                        .text.replace(\"\\n\", \" \").strip()\n",
    "                    title = driver.find_element_by_id('vjs-jobtitle')\\\n",
    "                        .text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "                    # Check if we already have a similar job offer in the df\n",
    "                    # with the same title, company, location and description\n",
    "                    # If it is the case, we pass to the next job offer\n",
    "                    if salary_df[(salary_df[\"Title\"] == title)\n",
    "                                 & (salary_df[\"Company\"] == company)\n",
    "                                 & (salary_df[\"Location\"] == location)\n",
    "                                 & (salary_df[\"Description\"] == job_desc)].empty:\n",
    "                        if profession == 'title%3A+data':\n",
    "                            job_search = \"Data\"\n",
    "                        else:\n",
    "                            job_search = \"Développeur\"\n",
    "\n",
    "                        job_offer = {'_id': id_,\n",
    "                                     'Title': title,\n",
    "                                     'Company': company,\n",
    "                                     'Location': location,\n",
    "                                     'Salary': salary,\n",
    "                                     'Description': job_desc,\n",
    "                                     'Date': date,\n",
    "                                     'Job_Search': job_search,\n",
    "                                     'Department_Search': dpt}\n",
    "\n",
    "                        try:\n",
    "                            # Insert into the MongoDB database\n",
    "                            job_offers_collec.insert_one(job_offer)\n",
    "                        except DuplicateKeyError:\n",
    "                            continue\n",
    "\n",
    "                        # Insert into the pandas DataFrame\n",
    "                        salary_df = salary_df.append(job_offer,\n",
    "                                                     ignore_index=True)\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # Click on the \"Suivant\" button :\n",
    "                try:\n",
    "                    if first_page:\n",
    "                        driver.find_element_by_class_name('np').click()\n",
    "                        first_page = False\n",
    "                    else:\n",
    "                        try:\n",
    "                            driver.find_elements_by_class_name('np')[1].click()\n",
    "                        except IndexError:\n",
    "                            # Last page, no \"Suivant\" button\n",
    "                            break\n",
    "                except NoSuchElementException:\n",
    "                    break\n",
    "\n",
    "            except ElementClickInterceptedException:\n",
    "                # If there is a popup, close it :\n",
    "                close_popup_button = driver.find_element_by_class_name(\n",
    "                                                'popover-x-button-close')\n",
    "                close_popup_button.click()\n",
    "                driver.implicitly_wait(4)\n",
    "\n",
    "salary_df.to_csv('salary_indeed.csv', index=False)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-16 11:27:29.808549\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tap in terminal : which python \n",
    "My Result --> (which python) /Users/fabi/Documents/Data AI Ecouen/PROJECTS/ML/TEST MAJ SCRAPING.ipynb >> ~/cron.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
